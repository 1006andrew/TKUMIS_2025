# uvicorn app:app --host 127.0.0.1 --port 8000 --reload

# ngrok http 8000 å•Ÿå‹•

from fastapi import FastAPI, Request, Form
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import json
import requests
import logging
from opencc import OpenCC
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from openai import OpenAI

# åˆå§‹åŒ– FastAPI
app = FastAPI()
templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# ä¸­æ–‡ç°¡ç¹è½‰æ›
cc_t2s = OpenCC('t2s')
cc_s2t = OpenCC('s2t')

# æ¥æ”¶å‰ç«¯å•é¡Œçš„æ ¼å¼


class QuestionRequest(BaseModel):
    question: str

# è®€å– JSON ä¸¦è½‰ç‚ºæ–‡æœ¬æ®µè½


def read_json_text(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)


def flatten_json_entries(data):
    chunks = []
    for entry in data:
        block = f"è†šè³ªé¡å‹ï¼š{entry['skin_type']}\n"
        for c in entry.get("recommended_courses", []):
            block += f"ç™‚ç¨‹ï¼š{c['name']}\nåŠŸæ•ˆï¼š{'ã€'.join(c.get('goals', []))}\n"
        for p in entry.get("products", []):
            block += f"ç”¢å“ï¼š{p['name']}ï¼ˆ{p['step']}ï¼‰\nç‰¹è‰²ï¼š{'ï¼›'.join(p.get('features', []))}\n"
        block += f"æ¨è–¦è©±è¡“ï¼š{entry.get('reply_template', '')}"
        chunks.append(block)
    return chunks


def embed_chunks(chunks, model):
    return model.encode(chunks)


def retrieve_top_k_chunks(query, chunks, embeddings, embed_model, k=3):
    query_embedding = embed_model.encode([query])
    sims = cosine_similarity(query_embedding, embeddings)[0]
    top_indices = np.argsort(sims)[-k:][::-1]
    return [chunks[i] for i in top_indices]


def query_ollama(context, question, model_name="llama3.1:8b"):
    messages = [{"role": "system", "content": """ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œã€æº«æŸ”ä¸”è¦ªåˆ‡çš„çš®è†šè­·ç†é¡§å•ï¼Œæ“…é•·æä¾›å°ˆæ¥­ä¸”æ˜“æ‡‚çš„å»ºè­°ã€‚ è«‹åš´æ ¼æ ¹æ“šã€åƒè€ƒè³‡æ–™ã€‘å’Œä½¿ç”¨è€…æœ€æ–°çš„å•é¡Œï¼Œä¾†ç”Ÿæˆä½ çš„å›ç­”ã€‚ ä½ çš„å›ç­”å¿…é ˆéµå¾ªä»¥ä¸‹è¦å‰‡ï¼š 1. **çµæ§‹åŒ–èˆ‡ç°¡æ½”æ€§**ï¼šå›ç­”å¿…é ˆä»¥æ¢åˆ—å¼å‘ˆç¾ï¼ˆä½¿ç”¨ 1. 2. 3. ...ï¼‰ã€‚ 2. **å…§å®¹ä¾†æº**ï¼šå›ç­”å…§å®¹**åš´æ ¼é™æ–¼ã€åƒè€ƒè³‡æ–™ã€‘å…§**ï¼Œä¸å¾—æ†‘ç©ºå‰µé€ ä»»ä½•ä¸å­˜åœ¨çš„ç”¢å“ã€ç™‚ç¨‹æˆ–è³‡è¨Šã€‚ 3. **å›ç­”æ¨¡å¼**ï¼š - **ç•¶ä½¿ç”¨è€…è©¢å•ç”¢å“æˆ–ç™‚ç¨‹é¡å‹ï¼ˆä¾‹å¦‚ï¼šç¾ç™½ã€ä¿æ¿•ï¼‰æ™‚**ï¼š - åˆ—å‡ºç›¸é—œç”¢å“æˆ–ç™‚ç¨‹çš„**åç¨±**å³å¯ã€‚ - ä¾‹å¦‚ï¼š ç¾ç™½ï¼š 1. ç…¥ç™½é€äº®è‡‰éƒ¨èª²ç¨‹ 2. å«©ç™½ç„¡æš‡ç²¾è¯æ¶² 3. ... - **ç•¶ä½¿ç”¨è€…è©¢å•å…·é«”çš„ç”¢å“æˆ–ç™‚ç¨‹ï¼ˆä¾‹å¦‚ï¼šæŸæŸèª²ç¨‹æœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿï¼‰æ™‚**ï¼š - è©³ç´°åˆ—å‡ºè©²ç”¢å“æˆ–ç™‚ç¨‹çš„**é‡é»ç‰¹è‰²**ï¼Œæ¯é …ä¸è¶…éä¸‰é»ã€‚ - ä¾‹å¦‚ï¼š ç…¥ç™½é€äº®è‡‰éƒ¨èª²ç¨‹ï¼š 1. æœ‰æ•ˆæ”¹å–„è‚Œè†šæš—æ²‰ï¼Œæ¢å¾©å…‰æ¾¤ã€‚ 2. æå‡è‚Œè†šæ°´æ½¤åº¦ï¼Œæ¸›å°‘ç´°ç´‹ã€‚ 3. èˆ’ç·©æ•æ„Ÿè‚Œè†šã€‚ 4. **èªè¨€èˆ‡æ ¼å¼**ï¼šæ‰€æœ‰å›ç­”éƒ½å¿…é ˆä½¿ç”¨**ç¹é«”ä¸­æ–‡**ï¼Œä¸¦ä¸”**ä¸åŒ…å«ä»»ä½•é–‹å ´ç™½æˆ–çµèª**ã€‚ """}, {"role": "user", "content": f""" ã€åƒè€ƒè³‡æ–™ã€‘ {context} ã€ä½¿ç”¨è€…æœ€æ–°å•é¡Œã€‘ {question} """}]
    try:
        res = requests.post("http://localhost:11434/api/chat", json={
            "model": model_name,
            "messages": messages,
            "stream": False
        })
        res.raise_for_status()
        return res.json()["message"]["content"]
    except Exception as e:
        logging.error(f"Ollama éŒ¯èª¤ï¼š{e}")
        return f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

def query_openai(context, question, model_name="gpt-4o-mini"):
    """
    ä½¿ç”¨ OpenAI ChatCompletion API é€²è¡Œå›ç­”
    """
    messages = [
        {
            "role": "system",
            "content": """
                    ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œã€æº«æŸ”ä¸”è¦ªåˆ‡çš„çš®è†šè­·ç†é¡§å•ï¼Œ
                    æ“…é•·æä¾›å°ˆæ¥­ä¸”æ˜“æ‡‚çš„å»ºè­°ã€‚
                    è«‹åš´æ ¼æ ¹æ“šã€åƒè€ƒè³‡æ–™ã€‘å’Œä½¿ç”¨è€…æœ€æ–°çš„å•é¡Œï¼Œä¾†ç”Ÿæˆä½ çš„å›ç­”ã€‚
                    ä½ çš„å›ç­”å¿…é ˆéµå¾ªä»¥ä¸‹è¦å‰‡ï¼š
                    1. **çµæ§‹åŒ–èˆ‡ç°¡æ½”æ€§**ï¼šå›ç­”å¿…é ˆä»¥æ¢åˆ—å¼å‘ˆç¾ï¼ˆä½¿ç”¨ 1. 2. 3. ...ï¼‰ã€‚
                    2. **å…§å®¹ä¾†æº**ï¼šå›ç­”å…§å®¹**åš´æ ¼é™æ–¼ã€åƒè€ƒè³‡æ–™ã€‘å…§**ï¼Œä¸å¾—æ†‘ç©ºå‰µé€ ä»»ä½•ä¸å­˜åœ¨çš„ç”¢å“ã€ç™‚ç¨‹æˆ–è³‡è¨Šã€‚
                    3. **å›ç­”æ¨¡å¼**ï¼š
                        - **ç•¶ä½¿ç”¨è€…è©¢å•ç”¢å“æˆ–ç™‚ç¨‹é¡å‹ï¼ˆä¾‹å¦‚ï¼šç¾ç™½ã€ä¿æ¿•ï¼‰æ™‚**ï¼š
                            - åˆ—å‡ºç›¸é—œç”¢å“æˆ–ç™‚ç¨‹çš„**åç¨±**å³å¯ã€‚
                        - **ç•¶ä½¿ç”¨è€…è©¢å•å…·é«”çš„ç”¢å“æˆ–ç™‚ç¨‹ï¼ˆä¾‹å¦‚ï¼šæŸæŸèª²ç¨‹æœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿï¼‰æ™‚**ï¼š
                            - è©³ç´°åˆ—å‡ºè©²ç”¢å“æˆ–ç™‚ç¨‹çš„**é‡é»ç‰¹è‰²**ï¼Œæ¯é …ä¸è¶…éä¸‰é»ã€‚
                    4. **èªè¨€èˆ‡æ ¼å¼**ï¼šæ‰€æœ‰å›ç­”éƒ½å¿…é ˆä½¿ç”¨**ç¹é«”ä¸­æ–‡**ï¼Œä¸¦ä¸”**ä¸åŒ…å«ä»»ä½•é–‹å ´ç™½æˆ–çµèª**ã€‚
        """
        },
        {
            "role": "user",
            "content": f"ã€åƒè€ƒè³‡æ–™ã€‘ {context}\nã€ä½¿ç”¨è€…æœ€æ–°å•é¡Œã€‘ {question}"
        }
    ]

    try:
        response = openai_client.chat.completions.create(
            model=model_name,
            messages=messages
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"OpenAI API éŒ¯èª¤ï¼š{e}")
        return f"âš ï¸ OpenAI API ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

# é è¼‰æ¨¡å‹èˆ‡è³‡æ–™
print("ğŸ”„ è¼‰å…¥è³‡æ–™èˆ‡æ¨¡å‹...")
data = read_json_text("skincare_data.json")
chunks = flatten_json_entries(data)
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
chunk_embeddings = embed_chunks(chunks, embed_model)
print("âœ… åˆå§‹åŒ–å®Œæˆï¼")

# ğŸ“Œ å‰ç«¯é¦–é 


@app.get("/")
def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# ğŸ“Œ æŸ¥è©¢ APIï¼ˆä¾› JS ä½¿ç”¨ï¼‰


@app.post("/chat")
def chat(request: QuestionRequest):
    question_trad = request.question
    question_simp = cc_t2s.convert(question_trad)

    # æª¢ç´¢ç›¸é—œå…§å®¹
    top_chunks = retrieve_top_k_chunks(
        question_simp, chunks, chunk_embeddings, embed_model)
    context = "\n---\n".join(top_chunks)

    # æŸ¥è©¢ Ollama
    answer_simp = query_ollama(context, question_trad)
    answer_trad = cc_s2t.convert(answer_simp)

    return {"answer": answer_trad}

from pyngrok import ngrok

if __name__ == "__main__":
    import uvicorn
    # # é–‹ä¸€å€‹ ngrok é€šé“
    # public_url = ngrok.connect(8000)
    # print("ğŸŒ Public URL:", public_url)
    # å•Ÿå‹• FastAPI
    uvicorn.run(app, host="0.0.0.0", port=8000)